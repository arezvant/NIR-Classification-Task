# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qZ5HcUJ28p1PQHzdsloFWIx6DgPMlA0J
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import os
import sys
drive.mount('/content/gdrive/', force_remount=True)
# %cd /content/gdrive/MyDrive/RetiSpec/Data
sys.path.insert(0,'/content/gdrive/MyDrive/RetiSpec')

!pip install mlflow
import mlflow
import mlflow.pytorch
import cv2, itertools
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from tqdm import tqdm
from glob import glob
from PIL import Image
from statistics import mean
from tifffile import TiffFile
from matplotlib.pyplot import figure
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
from torchvision import transforms
from configuration import config
from dataset.custom_data_loader import RetiSpecDataset
from models.eca_mobilenetv2 import ECA_MobileNetV2
from models.mobilenetv2 import MobileNetV2
from save_weight.save_checkpoint import save_checkpoint
import evaluate

!databricks configure --host https://community.cloud.databricks.com/
mlflow.set_tracking_uri("databricks")
mlflow.set_experiment("/Users/arezvant@gmail.com/RetiSpec")

def main (
    train_dir=None, val_dir=None, bs=16, arch='att-chan', lr=0.01,
     momentum=0.9, weight_decay = 5e-4, epochs=15):
  
  """
      arguments:

      train_dir: root to train set folder
      val_dir: root to validation set folder
      bs: default is 16
      arch: all architectures are based on MobileNetV2,
      architectures: default is att-chann 

      {
        att-chan (attention channel which utilizes 4 channels), 
        rgb-chan (rgb-channel which utilizes RGB channles), 
        nir-chan (rgb-channel which utilizes NIR channles),
        mixed-chan (mixed-channel which utilizes randomly 3 out of 4 channles)
        }

      lr: learning rate, default is 0.01
      momentum: default is 0.09
      weight_decay: default is 5e-4
      epochs: default is 15
  
  """

  # check to see if GPU is available and assign it to device
  def get_default_device():

    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')

  device = get_default_device()

  # initializing transformation options
  resize = transforms.Resize(size=(config.INPUT_HEIGHT, config.INPUT_WIDTH))
  h_flip = transforms.RandomHorizontalFlip(p=0.25)
  v_flip = transforms.RandomVerticalFlip(p=0.25)
  rotate = transforms.RandomRotation(degrees=15)
  pil = transforms.ToPILImage()
  BEST_ACC = 0

  # initialize training and validation set data augmentation pipeline
  train_transforms = transforms.Compose([pil, resize, h_flip, v_flip, rotate, 
                                        transforms.ToTensor()])
  val_transforms = transforms.Compose([pil, resize, transforms.ToTensor()])

  # number of samples insode training and validation folders
  train_folder = ImageFolder(root=train_dir)
  val_folder = ImageFolder(root=val_dir)
  print("[INFO] training folder contains {} samples...".format(
      len(train_folder)))
  print("[INFO] validation folder contains {} samples...".format(
      len(val_folder)))
  
  # initialize dataloader pipelines and their associated models
  print("[INFO] creating training and validation set dataloaders and models...")

  if arch == 'att-chan':
    rgb_n_train_dataset = RetiSpecDataset(train_dir)
    rgb_n_val_dataset = RetiSpecDataset(val_dir)
    train_loader = DataLoader(rgb_n_train_dataset, batch_size=bs, shuffle=True)
    val_loader = DataLoader(rgb_n_val_dataset, batch_size=bs, shuffle=True)
    model = ECA_MobileNetV2(
        num_classes=2, channels = 4, width_mult=1.0).to(device)

    criterion = nn.CrossEntropyLoss().to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr = 0.01,
                              momentum=0.9, weight_decay = 1e-4)
  
    losses = {'train':[], 'val':[]}
    accuracies = {'train':[], 'val':[]}
    for epoch in range(epochs):
        loop = tqdm(train_loader)
        epoch_loss = 0
        epoch_accuracy = 0
        print('Epoch: {}'.format(epoch + 1))
        for idx, (data, label) in enumerate(loop):
            data = data.to(device)
            label = label.to(device)
            output = model(data).to(device)
            loss = criterion(output, label.squeeze())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            acc = (output.argmax(1) == label.squeeze()).float().mean()
            epoch_accuracy += acc / len(train_loader)
            epoch_loss += loss / len(train_loader)
        accuracies['train'].append(epoch_accuracy)
        losses['train'].append(epoch_loss)
            
        with torch.no_grad():
            epoch_val_accuracy = 0
            epoch_val_loss = 0
            loop = tqdm(val_loader)
            for idx, (data, label) in enumerate(loop):
                data = data.to(device)
                label = label.to(device)
                val_output = model(data).to(device)
                val_loss = criterion(val_output, label.squeeze())
                acc = (val_output.argmax(1) == label.squeeze()).float().mean()
                epoch_val_accuracy += acc / len(val_loader)
                epoch_val_loss += val_loss / len(val_loader)
            accuracies['val'].append(epoch_val_accuracy)
            losses['val'].append(epoch_val_loss)
                
        print(
            f"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy*100:.2f} - "
            f"val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy*100:.2f}\n"
        )

        # remember best acc and save checkpoint"""
        is_best = epoch_val_accuracy > BEST_ACC
        BEST_ACC = max(epoch_val_accuracy, BEST_ACC)
        save_checkpoint({
            'state_dict': model.state_dict(),
            'best_acc': BEST_ACC,
            'optimizer': optimizer.state_dict(),
        }, is_best, model_name = arch)
        if is_best:
            print(f"NEW BEST WEIGHT WITH A VALIDATION ACCURACY OF : {BEST_ACC*100:.2f} IS SAVED.\n")
        else:
            print(f"Best weight: {BEST_ACC*100:.2f}\n")

        mlflow.end_run()
        with mlflow.start_run() as run:
          mlflow.log_param("Architecture", arch)
          mlflow.log_metric("Training Accuracy", epoch_accuracy*100)
          mlflow.log_metric("Validation Accuracy", epoch_val_accuracy*100)
          mlflow.pytorch.log_model(model, "models")

    evaluate.test(test_dir=val_dir, bs=16, arch='att-chan')

  elif arch == 'rgb-chan':
    rgb_train_dataset = RetiSpecDataset(train_dir, flag='rgb')
    rgb_val_dataset = RetiSpecDataset(val_dir, flag='rgb')
    rgb_train_loader = DataLoader(rgb_train_dataset, batch_size=bs, shuffle=True)
    rgb_val_loader = DataLoader(rgb_val_dataset, batch_size=bs, shuffle=True)
    model = MobileNetV2(
      num_classes = 2, channels = 3, width_mult = 1.0).to(device)

    criterion = nn.CrossEntropyLoss().to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr = 0.01,
                              momentum=0.9, weight_decay = 1e-4)
    
    losses = {'train':[], 'val':[]}
    accuracies = {'train':[], 'val':[]}
    for epoch in range(epochs):
        loop = tqdm(rgb_train_loader)
        epoch_loss = 0
        epoch_accuracy = 0
        print('Epoch: {}'.format(epoch + 1))
        for idx, (data, label) in enumerate(loop):
            data = data.to(device)
            label = label.to(device)
            output = model(data).to(device)
            loss = criterion(output, label.squeeze())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            acc = (output.argmax(1) == label.squeeze()).float().mean()
            epoch_accuracy += acc / len(rgb_train_loader)
            epoch_loss += loss / len(rgb_train_loader)
        accuracies['train'].append(epoch_accuracy)
        losses['train'].append(epoch_loss)
            
        with torch.no_grad():
            epoch_val_accuracy = 0
            epoch_val_loss = 0
            loop = tqdm(rgb_val_loader)
            for idx, (data, label) in enumerate(loop):
                data = data.to(device)
                label = label.to(device)
                val_output = model(data).to(device)
                val_loss = criterion(val_output, label.squeeze())
                acc = (val_output.argmax(1) == label.squeeze()).float().mean()
                epoch_val_accuracy += acc / len(rgb_val_loader)
                epoch_val_loss += val_loss / len(rgb_val_loader)
            accuracies['val'].append(epoch_val_accuracy)
            losses['val'].append(epoch_val_loss)
                
        print(
            f"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy*100:.2f} - "
            f"val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy*100:.2f}\n"
        )

        # remember best acc and save checkpoint"""
        is_best = epoch_val_accuracy > BEST_ACC
        BEST_ACC = max(epoch_val_accuracy, BEST_ACC)
        save_checkpoint({
            'state_dict': model.state_dict(),
            'best_acc': BEST_ACC,
            'optimizer': optimizer.state_dict(),
        }, is_best, model_name = arch)
        if is_best:
            print(f"NEW BEST WEIGHT WITH A VALIDATION ACCURACY OF : {BEST_ACC*100:.2f} IS SAVED.\n")
        else:
            print(f"Best weight: {BEST_ACC*100:.2f}\n")

        mlflow.end_run()
        with mlflow.start_run() as run:
          mlflow.log_param("Architecture", arch)
          mlflow.log_metric("Training Accuracy", epoch_accuracy*100)
          mlflow.log_metric("Validation Accuracy", epoch_val_accuracy*100)
          mlflow.pytorch.log_model(model, "models")

    evaluate.test(test_dir=val_dir, bs=16, arch='rgb-chan')

  elif arch == 'nir-chan':
    N_train_dataset = RetiSpecDataset(train_dir, flag='nir')
    N_val_dataset = RetiSpecDataset(val_dir, flag='nir')
    N_train_loader = DataLoader(N_train_dataset, batch_size=bs, shuffle=True)
    N_val_loader = DataLoader(N_val_dataset, batch_size=bs, shuffle=True)

    model = MobileNetV2(
      num_classes = 2, channels = 3, width_mult = 1.0).to(device)

    criterion = nn.CrossEntropyLoss().to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr = 0.01,
                              momentum=0.9, weight_decay = 5e-4)
    losses = {'train':[], 'val':[]}
    accuracies = {'train':[], 'val':[]}
    for epoch in range(epochs):
        loop = tqdm(N_train_loader)
        epoch_loss = 0
        epoch_accuracy = 0
        print('Epoch: {}'.format(epoch + 1))
        for idx, (data, label) in enumerate(loop):
            data = data.to(device)
            label = label.to(device)
            output = model(data).to(device)
            loss = criterion(output, label.squeeze())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            acc = (output.argmax(1) == label.squeeze()).float().mean()
            epoch_accuracy += acc / len(N_train_loader)
            epoch_loss += loss / len(N_train_loader)
        accuracies['train'].append(epoch_accuracy)
        losses['train'].append(epoch_loss)
            
        with torch.no_grad():
            epoch_val_accuracy = 0
            epoch_val_loss = 0
            loop = tqdm(N_val_loader)
            for idx, (data, label) in enumerate(loop):
                data = data.to(device)
                label = label.to(device)
                val_output = model(data).to(device)
                val_loss = criterion(val_output, label.squeeze())
                acc = (val_output.argmax(1) == label.squeeze()).float().mean()
                epoch_val_accuracy += acc / len(N_val_loader)
                epoch_val_loss += val_loss / len(N_val_loader)
            accuracies['val'].append(epoch_val_accuracy)
            losses['val'].append(epoch_val_loss)
                
        print(
            f"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy*100:.2f} - "
            f"val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy*100:.2f}\n"
        )

        # remember best acc and save checkpoint"""
        is_best = epoch_val_accuracy > BEST_ACC
        BEST_ACC = max(epoch_val_accuracy, BEST_ACC)
        save_checkpoint({
            'state_dict': model.state_dict(),
            'best_acc': BEST_ACC,
            'optimizer': optimizer.state_dict(),
        }, is_best, model_name = arch)
        if is_best:
            print(f"NEW BEST WEIGHT WITH A VALIDATION ACCURACY OF : {BEST_ACC*100:.2f} IS SAVED.\n")
        else:
            print(f"Best weight: {BEST_ACC*100:.2f}\n")

        mlflow.end_run()
        with mlflow.start_run() as run:
          mlflow.log_param("Architecture", arch)
          mlflow.log_metric("Training Accuracy", epoch_accuracy*100)
          mlflow.log_metric("Validation Accuracy", epoch_val_accuracy*100)
          mlflow.pytorch.log_model(model, "models")

    evaluate.test(test_dir=val_dir, bs=16, arch='nir-chan')

  else: #mixed-chan scenario
    mixed_train_dataset = RetiSpecDataset(train_dir, flag = 'mix')
    mixed_val_dataset = RetiSpecDataset(val_dir, flag = 'mix')
    train_loader = DataLoader(mixed_train_dataset, batch_size=bs, shuffle=True)
    val_loader = DataLoader(mixed_val_dataset, batch_size=bs, shuffle=True)
    model = MobileNetV2(
      num_classes = 2, channels = 4, width_mult = 1.0).to(device)

    criterion = nn.CrossEntropyLoss().to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr = 0.01,
                              momentum=0.9, weight_decay = 5e-4)
  
    losses = {'train':[], 'val':[]}
    accuracies = {'train':[], 'val':[]}
    for epoch in range(epochs):
        loop = tqdm(train_loader)
        epoch_loss = 0
        epoch_accuracy = 0
        print('Epoch: {}'.format(epoch + 1))
        for idx, (data, label) in enumerate(loop):
            data = data.to(device)
            label = label.to(device)
            output = model(data).to(device)
            loss = criterion(output, label.squeeze())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            acc = (output.argmax(1) == label.squeeze()).float().mean()
            epoch_accuracy += acc / len(train_loader)
            epoch_loss += loss / len(train_loader)
        accuracies['train'].append(epoch_accuracy)
        losses['train'].append(epoch_loss)
            
        with torch.no_grad():
            epoch_val_accuracy = 0
            epoch_val_loss = 0
            loop = tqdm(val_loader)
            for idx, (data, label) in enumerate(loop):
                data = data.to(device)
                label = label.to(device)
                val_output = model(data).to(device)
                val_loss = criterion(val_output, label.squeeze())
                acc = (val_output.argmax(1) == label.squeeze()).float().mean()
                epoch_val_accuracy += acc / len(val_loader)
                epoch_val_loss += val_loss / len(val_loader)
            accuracies['val'].append(epoch_val_accuracy)
            losses['val'].append(epoch_val_loss)
                
        print(
            f"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy*100:.2f} - "
            f"val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy*100:.2f}\n"
        )

        # remember best acc and save checkpoint"""
        is_best = epoch_val_accuracy > BEST_ACC
        BEST_ACC = max(epoch_val_accuracy, BEST_ACC)
        save_checkpoint({
            'state_dict': model.state_dict(),
            'best_acc': BEST_ACC,
            'optimizer': optimizer.state_dict(),
        }, is_best, model_name = arch)
        if is_best:
            print(f"NEW BEST WEIGHT WITH A VALIDATION ACCURACY OF : {BEST_ACC*100:.2f} IS SAVED.\n")
        else:
            print(f"Best weight: {BEST_ACC*100:.2f}\n")

        mlflow.end_run()
        with mlflow.start_run() as run:
          mlflow.log_param("Architecture", arch)
          mlflow.log_metric("Training Accuracy", epoch_accuracy*100)
          mlflow.log_metric("Validation Accuracy", epoch_val_accuracy*100)
          mlflow.pytorch.log_model(model, "models")

    evaluate.test(test_dir=val_dir, bs=16, arch='mixed-chan')

if __name__ == '__main__':

  mlflow.set_experiment(experiment_name = "/Users/arezvant@gmail.com/RetiSpec")
  tr_root = config.RetiSpec_DATASET_PATH + '/train'
  val_root = config.RetiSpec_DATASET_PATH + '/val'

  main(train_dir=tr_root, val_dir=val_root, bs=16, arch='nir-chan', lr=0.01, 
       momentum=0.9, weight_decay = 1e-4, epochs=10)